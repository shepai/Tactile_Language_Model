{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52fa312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c2113d",
   "metadata": {},
   "source": [
    "# Convert data to format\n",
    "\n",
    "- images to temporal images\n",
    "- text to encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "683fd94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((3,3,50,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a439947",
   "metadata": {},
   "source": [
    "String conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04d0bfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 8678,  1774,     1,  ...,     0,     0,     0],\n",
      "        [13441,   574,     1,  ...,     0,     0,     0],\n",
      "        [ 3735, 20382,    53,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "tensor([[ 8678,  1774,     1,  ...,     0,     0,     0],\n",
      "        [13441,   574,     1,  ...,     0,     0,     0],\n",
      "        [ 3735, 20382,    53,  ...,     0,     0,     0]])\n",
      "rough surface\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")  # or \"gpt2\", etc.\n",
    "\n",
    "text = [\"rough surface\", \"sliding contact\", \"object tilting\"]\n",
    "tokenized = tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "print(tokenized)\n",
    "input_ids=tokenized['input_ids']\n",
    "print(input_ids)  \n",
    "decoded=tokenizer.decode(input_ids[0].squeeze(), skip_special_tokens=True)\n",
    "print(decoded)\n",
    "y=decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d565ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_captions(captions, max_len=30):\n",
    "    return tokenizer(\n",
    "        captions,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54731eb",
   "metadata": {},
   "source": [
    "# Train model to predict text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2201d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TactileDataset(Dataset):\n",
    "    def __init__(self, images, captions, tokenizer, max_len=30):\n",
    "        self.images = images\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.captions = captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        caption = self.captions[idx]\n",
    "\n",
    "        tokens = self.tokenizer(caption, max_length=self.max_len,\n",
    "                                padding=\"max_length\", truncation=True,\n",
    "                                return_tensors=\"pt\")\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"input_ids\": tokens[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokens[\"attention_mask\"].squeeze(0)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "285e24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TactileCaptioningModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, max_len=30):\n",
    "        super().__init__()\n",
    "        # Image encoder: ResNet18 without classifier\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        resnet.fc = nn.Identity()\n",
    "        self.encoder = resnet\n",
    "\n",
    "        # Project image features\n",
    "        self.img_proj = nn.Linear(512, embed_dim)\n",
    "\n",
    "        # Text decoder: Embedding + Transformer Decoder\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=4)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=2)\n",
    "\n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, images, input_ids):\n",
    "        batch_size = images.size(0)\n",
    "        image_features = self.encoder(images)  # (B, 512)\n",
    "        image_features = self.img_proj(image_features)  # (B, E)\n",
    "        image_features = image_features.unsqueeze(1)  # (B, 1, E)\n",
    "\n",
    "        tgt = self.embedding(input_ids)  # (B, T, E)\n",
    "        tgt = tgt.transpose(0, 1)  # (T, B, E)\n",
    "        memory = image_features.transpose(0, 1)  # (1, B, E)\n",
    "\n",
    "        out = self.transformer_decoder(tgt, memory)  # (T, B, E)\n",
    "        out = out.transpose(0, 1)  # (B, T, E)\n",
    "\n",
    "        return self.output_layer(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "903c9f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 10.8935\n",
      "Epoch 1: Loss = 9.8110\n",
      "Epoch 2: Loss = 8.7889\n",
      "Epoch 3: Loss = 8.0141\n",
      "Epoch 4: Loss = 7.2723\n",
      "Epoch 5: Loss = 6.5863\n",
      "Epoch 6: Loss = 6.1406\n",
      "Epoch 7: Loss = 5.6804\n",
      "Epoch 8: Loss = 5.4007\n",
      "Epoch 9: Loss = 4.9795\n",
      "Epoch 10: Loss = 4.8030\n",
      "Epoch 11: Loss = 4.5864\n",
      "Epoch 12: Loss = 4.4110\n",
      "Epoch 13: Loss = 4.1720\n",
      "Epoch 14: Loss = 4.0395\n",
      "Epoch 15: Loss = 4.0044\n",
      "Epoch 16: Loss = 3.8023\n",
      "Epoch 17: Loss = 3.7459\n",
      "Epoch 18: Loss = 3.5053\n",
      "Epoch 19: Loss = 3.4694\n",
      "Epoch 20: Loss = 3.2419\n",
      "Epoch 21: Loss = 3.2111\n",
      "Epoch 22: Loss = 3.0373\n",
      "Epoch 23: Loss = 2.9363\n",
      "Epoch 24: Loss = 2.8049\n",
      "Epoch 25: Loss = 2.6922\n",
      "Epoch 26: Loss = 2.5326\n",
      "Epoch 27: Loss = 2.4030\n",
      "Epoch 28: Loss = 2.2992\n",
      "Epoch 29: Loss = 2.2733\n",
      "Epoch 30: Loss = 1.9953\n",
      "Epoch 31: Loss = 2.0527\n",
      "Epoch 32: Loss = 1.9550\n",
      "Epoch 33: Loss = 1.8417\n",
      "Epoch 34: Loss = 1.6781\n",
      "Epoch 35: Loss = 1.6407\n",
      "Epoch 36: Loss = 1.5731\n",
      "Epoch 37: Loss = 1.6151\n",
      "Epoch 38: Loss = 1.4555\n",
      "Epoch 39: Loss = 1.3644\n",
      "Epoch 40: Loss = 1.4308\n",
      "Epoch 41: Loss = 1.3380\n",
      "Epoch 42: Loss = 1.2436\n",
      "Epoch 43: Loss = 1.2052\n",
      "Epoch 44: Loss = 1.1594\n",
      "Epoch 45: Loss = 1.1292\n",
      "Epoch 46: Loss = 1.0885\n",
      "Epoch 47: Loss = 1.0718\n",
      "Epoch 48: Loss = 1.0305\n",
      "Epoch 49: Loss = 0.9886\n",
      "Epoch 50: Loss = 0.9373\n",
      "Epoch 51: Loss = 0.9004\n",
      "Epoch 52: Loss = 0.8991\n",
      "Epoch 53: Loss = 0.8958\n",
      "Epoch 54: Loss = 0.9206\n",
      "Epoch 55: Loss = 0.9552\n",
      "Epoch 56: Loss = 0.8739\n",
      "Epoch 57: Loss = 0.7902\n",
      "Epoch 58: Loss = 0.9020\n",
      "Epoch 59: Loss = 0.8182\n",
      "Epoch 60: Loss = 0.7244\n",
      "Epoch 61: Loss = 0.7339\n",
      "Epoch 62: Loss = 0.6993\n",
      "Epoch 63: Loss = 0.6873\n",
      "Epoch 64: Loss = 0.6967\n",
      "Epoch 65: Loss = 0.6431\n",
      "Epoch 66: Loss = 0.6799\n",
      "Epoch 67: Loss = 0.6330\n",
      "Epoch 68: Loss = 0.6314\n",
      "Epoch 69: Loss = 0.6214\n",
      "Epoch 70: Loss = 0.6008\n",
      "Epoch 71: Loss = 0.5882\n",
      "Epoch 72: Loss = 0.5557\n",
      "Epoch 73: Loss = 0.5545\n",
      "Epoch 74: Loss = 0.5575\n",
      "Epoch 75: Loss = 0.5526\n",
      "Epoch 76: Loss = 0.5221\n",
      "Epoch 77: Loss = 0.5551\n",
      "Epoch 78: Loss = 0.5041\n",
      "Epoch 79: Loss = 0.4928\n",
      "Epoch 80: Loss = 0.4717\n",
      "Epoch 81: Loss = 0.4712\n",
      "Epoch 82: Loss = 0.4927\n",
      "Epoch 83: Loss = 0.4739\n",
      "Epoch 84: Loss = 0.4603\n",
      "Epoch 85: Loss = 0.4498\n",
      "Epoch 86: Loss = 0.4351\n",
      "Epoch 87: Loss = 0.4374\n",
      "Epoch 88: Loss = 0.4135\n",
      "Epoch 89: Loss = 0.4104\n",
      "Epoch 90: Loss = 0.4088\n",
      "Epoch 91: Loss = 0.4159\n",
      "Epoch 92: Loss = 0.3909\n",
      "Epoch 93: Loss = 0.4078\n",
      "Epoch 94: Loss = 0.4015\n",
      "Epoch 95: Loss = 0.3798\n",
      "Epoch 96: Loss = 0.3564\n",
      "Epoch 97: Loss = 0.3524\n",
      "Epoch 98: Loss = 0.3733\n",
      "Epoch 99: Loss = 0.3635\n"
     ]
    }
   ],
   "source": [
    "model = TactileCaptioningModel(vocab_size=tokenizer.vocab_size)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "dataset = TactileDataset(X, y, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "\n",
    "        outputs = model(images, input_ids[:, :-1])\n",
    "        loss = loss_fn(outputs.reshape(-1, tokenizer.vocab_size), input_ids[:, 1:].reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch}: Loss = {total_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "449f6aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, image, tokenizer, max_len=30, device='cpu'):\n",
    "    model.eval()\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Step 1: Encode image\n",
    "        image_features = model.encoder(image)  # (1, 512)\n",
    "        image_features = model.img_proj(image_features).unsqueeze(1).transpose(0, 1)  # (1, 1, embed_dim)\n",
    "\n",
    "        # Step 2: Start decoding\n",
    "        input_ids = torch.tensor([[tokenizer.pad_token_id]], device=device)  # (1, 1)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            tgt = model.embedding(input_ids)  # (1, T, E)\n",
    "            tgt = tgt.transpose(0, 1)         # (T, 1, E)\n",
    "\n",
    "            output = model.transformer_decoder(tgt, image_features)  # (T, 1, E)\n",
    "            last_token_logits = model.output_layer(output[-1])       # (1, vocab_size)\n",
    "            next_token_id = torch.argmax(last_token_logits, dim=-1)  # (1,)\n",
    "\n",
    "            # Append to sequence\n",
    "            input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=1)\n",
    "\n",
    "            # Stop if EOS\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        decoded = tokenizer.decode(input_ids.squeeze(), skip_special_tokens=True)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "903c9d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted caption: \n"
     ]
    }
   ],
   "source": [
    "caption = generate_caption(model, torch.rand(1,3,10,50), tokenizer, max_len=30, device=device)\n",
    "print(\"Predicted caption:\", caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ec9f61",
   "metadata": {},
   "source": [
    "# Use the text with another LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89daf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
