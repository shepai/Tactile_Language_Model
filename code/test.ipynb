{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52fa312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c2113d",
   "metadata": {},
   "source": [
    "# Convert data to format\n",
    "\n",
    "- images to temporal images\n",
    "- text to encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683fd94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"/its/home/drs25/Documents/data/Tactile Dataset/datasets/X_data_15.npz\")['arr_0']\n",
    "y = np.load(\"/its/home/drs25/Documents/data/Tactile Dataset/datasets/y_data_15.npz\")['arr_0']\n",
    "print(X)\n",
    "keys=['Carpet', 'LacedMatt', 'wool', 'Cork', 'Felt', 'LongCarpet', 'cotton', 'Plastic', 'Flat', 'Ffoam', 'Gfoam', 'bubble', 'Efoam', 'jeans', 'Leather']\n",
    "material_descriptions = {\n",
    "    \"Carpet\": \"Dense, woven fibers, soft yet coarse, typically synthetic or wool blend.\",\n",
    "    \"LacedMatt\": \"Light, airy mesh structure with interwoven hard lace patterns; bumpy and flexible.\",\n",
    "    \"Wool\": \"Natural fiber, soft, warm, and slightly scratchy; high friction texture.\",\n",
    "    \"Cork\": \"Lightweight, firm but compressible, slightly rough with granular texture.\",\n",
    "    \"Felt\": \"Compressed fabric, soft and smooth surface, uniform texture with slight give.\",\n",
    "    \"LongCarpet\": \"High-pile carpet with long fibers, soft and plush, deep texture.\",\n",
    "    \"Cotton\": \"Smooth and soft woven fabric, breathable with moderate friction.\",\n",
    "    \"Plastic\": \"Hard, smooth surface with low friction; can vary from rigid to flexible.\",\n",
    "    \"Flat\": \"Smooth and even surface, minimal texture; likely hard or semi-soft material.\",\n",
    "    \"Ffoam\": \"Soft with slight springiness, absorbs pressure well.\",\n",
    "    \"Gfoam\": \"Grainy foam, slightly rougher texture, spongy and compressible.\",\n",
    "    \"Bubble\": \"Bubble wrap or bubbled plastic, soft with raised circular nodes, very bumpy.\",\n",
    "    \"Efoam\": \"Soft with slight springiness, absorbs pressure well.\",\n",
    "    \"Jeans\": \"Sturdy cotton denim, rough woven texture, moderate friction.\",\n",
    "    \"Leather\": \"Smooth and durable natural material, slightly soft with subtle grain.\"\n",
    "}\n",
    "new_y=[]\n",
    "for i in range(len(y)):\n",
    "    label=keys[y[i]]\n",
    "    new_y.append(material_descriptions[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a439947",
   "metadata": {},
   "source": [
    "String conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04d0bfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 8678,  1774,     1,  ...,     0,     0,     0],\n",
      "        [13441,   574,     1,  ...,     0,     0,     0],\n",
      "        [ 3735, 20382,    53,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "tensor([[ 8678,  1774,     1,  ...,     0,     0,     0],\n",
      "        [13441,   574,     1,  ...,     0,     0,     0],\n",
      "        [ 3735, 20382,    53,  ...,     0,     0,     0]])\n",
      "rough surface\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")  # or \"gpt2\", etc.\n",
    "\n",
    "text = new_y\n",
    "tokenized = tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "print(tokenized)\n",
    "input_ids=tokenized['input_ids']\n",
    "print(input_ids)  \n",
    "decoded=tokenizer.decode(input_ids[0].squeeze(), skip_special_tokens=True)\n",
    "print(decoded)\n",
    "y=decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d565ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_captions(captions, max_len=30):\n",
    "    return tokenizer(\n",
    "        captions,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54731eb",
   "metadata": {},
   "source": [
    "# Train model to predict text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2201d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TactileDataset(Dataset):\n",
    "    def __init__(self, images, captions, tokenizer, max_len=30):\n",
    "        self.images = images\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.captions = captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        caption = self.captions[idx]\n",
    "\n",
    "        tokens = self.tokenizer(caption, max_length=self.max_len,\n",
    "                                padding=\"max_length\", truncation=True,\n",
    "                                return_tensors=\"pt\")\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"input_ids\": tokens[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokens[\"attention_mask\"].squeeze(0)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "285e24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TactileCaptioningModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, max_len=30):\n",
    "        super().__init__()\n",
    "        # Image encoder: ResNet18 without classifier\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        resnet.fc = nn.Identity()\n",
    "        self.encoder = resnet\n",
    "\n",
    "        # Project image features\n",
    "        self.img_proj = nn.Linear(512, embed_dim)\n",
    "\n",
    "        # Text decoder: Embedding + Transformer Decoder\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=4)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=2)\n",
    "\n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, images, input_ids):\n",
    "        batch_size = images.size(0)\n",
    "        image_features = self.encoder(images)  # (B, 512)\n",
    "        image_features = self.img_proj(image_features)  # (B, E)\n",
    "        image_features = image_features.unsqueeze(1)  # (B, 1, E)\n",
    "\n",
    "        tgt = self.embedding(input_ids)  # (B, T, E)\n",
    "        tgt = tgt.transpose(0, 1)  # (T, B, E)\n",
    "        memory = image_features.transpose(0, 1)  # (1, B, E)\n",
    "\n",
    "        out = self.transformer_decoder(tgt, memory)  # (T, B, E)\n",
    "        out = out.transpose(0, 1)  # (B, T, E)\n",
    "\n",
    "        return self.output_layer(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "903c9f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 9.8589\n",
      "Epoch 1: Loss = 9.0326\n",
      "Epoch 2: Loss = 8.0149\n",
      "Epoch 3: Loss = 7.3143\n",
      "Epoch 4: Loss = 6.6493\n",
      "Epoch 5: Loss = 6.1983\n",
      "Epoch 6: Loss = 5.7620\n",
      "Epoch 7: Loss = 5.4083\n",
      "Epoch 8: Loss = 5.1186\n",
      "Epoch 9: Loss = 4.8064\n",
      "Epoch 10: Loss = 4.6610\n",
      "Epoch 11: Loss = 4.4525\n",
      "Epoch 12: Loss = 4.2093\n",
      "Epoch 13: Loss = 4.1113\n",
      "Epoch 14: Loss = 3.9484\n",
      "Epoch 15: Loss = 3.7362\n",
      "Epoch 16: Loss = 3.7958\n",
      "Epoch 17: Loss = 3.4367\n",
      "Epoch 18: Loss = 3.3627\n",
      "Epoch 19: Loss = 3.1634\n",
      "Epoch 20: Loss = 3.0672\n",
      "Epoch 21: Loss = 2.7976\n",
      "Epoch 22: Loss = 2.7529\n",
      "Epoch 23: Loss = 2.5470\n",
      "Epoch 24: Loss = 2.5318\n",
      "Epoch 25: Loss = 2.3533\n",
      "Epoch 26: Loss = 2.2133\n",
      "Epoch 27: Loss = 2.1304\n",
      "Epoch 28: Loss = 1.9935\n",
      "Epoch 29: Loss = 1.9271\n",
      "Epoch 30: Loss = 1.8849\n",
      "Epoch 31: Loss = 1.7960\n",
      "Epoch 32: Loss = 1.7898\n",
      "Epoch 33: Loss = 1.6610\n",
      "Epoch 34: Loss = 1.5369\n",
      "Epoch 35: Loss = 1.5592\n",
      "Epoch 36: Loss = 1.4944\n",
      "Epoch 37: Loss = 1.4062\n",
      "Epoch 38: Loss = 1.3820\n",
      "Epoch 39: Loss = 1.3105\n",
      "Epoch 40: Loss = 1.3287\n",
      "Epoch 41: Loss = 1.4044\n",
      "Epoch 42: Loss = 1.1859\n",
      "Epoch 43: Loss = 1.2339\n",
      "Epoch 44: Loss = 1.1428\n",
      "Epoch 45: Loss = 1.1392\n",
      "Epoch 46: Loss = 1.0938\n",
      "Epoch 47: Loss = 1.0329\n",
      "Epoch 48: Loss = 1.0472\n",
      "Epoch 49: Loss = 1.0425\n",
      "Epoch 50: Loss = 0.9689\n",
      "Epoch 51: Loss = 0.9601\n",
      "Epoch 52: Loss = 0.9004\n",
      "Epoch 53: Loss = 0.9444\n",
      "Epoch 54: Loss = 0.8719\n",
      "Epoch 55: Loss = 0.8477\n",
      "Epoch 56: Loss = 0.8429\n",
      "Epoch 57: Loss = 0.8606\n",
      "Epoch 58: Loss = 0.8413\n",
      "Epoch 59: Loss = 0.7978\n",
      "Epoch 60: Loss = 0.8134\n",
      "Epoch 61: Loss = 0.7662\n",
      "Epoch 62: Loss = 0.7704\n",
      "Epoch 63: Loss = 0.7237\n",
      "Epoch 64: Loss = 0.7451\n",
      "Epoch 65: Loss = 0.6834\n",
      "Epoch 66: Loss = 0.6745\n",
      "Epoch 67: Loss = 0.6745\n",
      "Epoch 68: Loss = 0.7373\n",
      "Epoch 69: Loss = 0.6835\n",
      "Epoch 70: Loss = 0.6177\n",
      "Epoch 71: Loss = 0.6584\n",
      "Epoch 72: Loss = 0.5934\n",
      "Epoch 73: Loss = 0.5944\n",
      "Epoch 74: Loss = 0.6256\n",
      "Epoch 75: Loss = 0.5546\n",
      "Epoch 76: Loss = 0.5486\n",
      "Epoch 77: Loss = 0.5867\n",
      "Epoch 78: Loss = 0.5454\n",
      "Epoch 79: Loss = 0.5844\n",
      "Epoch 80: Loss = 0.5078\n",
      "Epoch 81: Loss = 0.5137\n",
      "Epoch 82: Loss = 0.4907\n",
      "Epoch 83: Loss = 0.4898\n",
      "Epoch 84: Loss = 0.4689\n",
      "Epoch 85: Loss = 0.4579\n",
      "Epoch 86: Loss = 0.4938\n",
      "Epoch 87: Loss = 0.4632\n",
      "Epoch 88: Loss = 0.4384\n",
      "Epoch 89: Loss = 0.4506\n",
      "Epoch 90: Loss = 0.4318\n",
      "Epoch 91: Loss = 0.4093\n",
      "Epoch 92: Loss = 0.4123\n",
      "Epoch 93: Loss = 0.4129\n",
      "Epoch 94: Loss = 0.3947\n",
      "Epoch 95: Loss = 0.3923\n",
      "Epoch 96: Loss = 0.3751\n",
      "Epoch 97: Loss = 0.3896\n",
      "Epoch 98: Loss = 0.3927\n",
      "Epoch 99: Loss = 0.3500\n"
     ]
    }
   ],
   "source": [
    "model = TactileCaptioningModel(vocab_size=tokenizer.vocab_size)\n",
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "dataset = TactileDataset(X, y, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "\n",
    "        outputs = model(images, input_ids[:, :-1])\n",
    "        loss = loss_fn(outputs.reshape(-1, tokenizer.vocab_size), input_ids[:, 1:].reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch}: Loss = {total_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "449f6aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, image, tokenizer, max_len=30, device='cpu'):\n",
    "    model.eval()\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Step 1: Encode image\n",
    "        image_features = model.encoder(image)  # (1, 512)\n",
    "        image_features = model.img_proj(image_features).unsqueeze(1).transpose(0, 1)  # (1, 1, embed_dim)\n",
    "\n",
    "        # Step 2: Start decoding\n",
    "        input_ids = torch.tensor([[tokenizer.pad_token_id]], device=device)  # (1, 1)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            tgt = model.embedding(input_ids)  # (1, T, E)\n",
    "            tgt = tgt.transpose(0, 1)         # (T, 1, E)\n",
    "\n",
    "            output = model.transformer_decoder(tgt, image_features)  # (T, 1, E)\n",
    "            last_token_logits = model.output_layer(output[-1])       # (1, vocab_size)\n",
    "            next_token_id = torch.argmax(last_token_logits, dim=-1)  # (1,)\n",
    "\n",
    "            # Append to sequence\n",
    "            input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=1)\n",
    "\n",
    "            # Stop if EOS\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        decoded = tokenizer.decode(input_ids.squeeze(), skip_special_tokens=True)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "903c9d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted caption: \n"
     ]
    }
   ],
   "source": [
    "caption = generate_caption(model, torch.rand(1,3,10,50), tokenizer, max_len=30, device=device)\n",
    "print(\"Predicted caption:\", caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ec9f61",
   "metadata": {},
   "source": [
    "# Use the text with another LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89daf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
