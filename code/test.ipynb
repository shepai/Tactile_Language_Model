{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27c2113d",
   "metadata": {},
   "source": [
    "# Convert data to format\n",
    "\n",
    "- images to temporal images\n",
    "- text to encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a439947",
   "metadata": {},
   "source": [
    "String conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d0bfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 8678,  1774,     1,  ...,     0,     0,     0],\n",
      "        [13441,   574,     1,  ...,     0,     0,     0],\n",
      "        [ 3735, 20382,    53,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "tensor([[ 8678,  1774,     1,  ...,     0,     0,     0],\n",
      "        [13441,   574,     1,  ...,     0,     0,     0],\n",
      "        [ 3735, 20382,    53,  ...,     0,     0,     0]])\n",
      "rough surface\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")  # or \"gpt2\", etc.\n",
    "\n",
    "text = [\"rough surface\", \"sliding contact\", \"object tilting\"]\n",
    "tokenized = tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "print(tokenized)\n",
    "input_ids=tokenized['input_ids']\n",
    "print(input_ids)  \n",
    "decoded=tokenizer.decode(input_ids[0].squeeze(), skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d565ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_captions(captions, max_len=30):\n",
    "    return tokenizer(\n",
    "        captions,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54731eb",
   "metadata": {},
   "source": [
    "# Train model to predict text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2201d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TactileDataset(Dataset):\n",
    "    def __init__(self, images, captions, tokenizer, max_len=30):\n",
    "        self.images = images\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.captions = captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        caption = self.captions[idx]\n",
    "\n",
    "        tokens = self.tokenizer(caption, max_length=self.max_len,\n",
    "                                padding=\"max_length\", truncation=True,\n",
    "                                return_tensors=\"pt\")\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"input_ids\": tokens[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokens[\"attention_mask\"].squeeze(0)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "285e24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TactileCaptioningModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, max_len=30):\n",
    "        super().__init__()\n",
    "        # Image encoder: ResNet18 without classifier\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        resnet.fc = nn.Identity()\n",
    "        self.encoder = resnet\n",
    "\n",
    "        # Project image features\n",
    "        self.img_proj = nn.Linear(512, embed_dim)\n",
    "\n",
    "        # Text decoder: Embedding + Transformer Decoder\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=4)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=2)\n",
    "\n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, images, input_ids):\n",
    "        batch_size = images.size(0)\n",
    "        image_features = self.encoder(images)  # (B, 512)\n",
    "        image_features = self.img_proj(image_features)  # (B, E)\n",
    "        image_features = image_features.unsqueeze(1)  # (B, 1, E)\n",
    "\n",
    "        tgt = self.embedding(input_ids)  # (B, T, E)\n",
    "        tgt = tgt.transpose(0, 1)  # (T, B, E)\n",
    "        memory = image_features.transpose(0, 1)  # (1, B, E)\n",
    "\n",
    "        out = self.transformer_decoder(tgt, memory)  # (T, B, E)\n",
    "        out = out.transpose(0, 1)  # (B, T, E)\n",
    "\n",
    "        return self.output_layer(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c9f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TactileCaptioningModel(vocab_size=tokenizer.vocab_size)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "dataset = TactileDataset(X, y, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "\n",
    "        outputs = model(images, input_ids[:, :-1])\n",
    "        loss = loss_fn(outputs.reshape(-1, tokenizer.vocab_size), input_ids[:, 1:].reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch}: Loss = {total_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ec9f61",
   "metadata": {},
   "source": [
    "# Use the text with another LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89daf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
